(window.webpackJsonp=window.webpackJsonp||[]).push([[220],{297:function(e,n,t){"use strict";t.r(n),t.d(n,"frontMatter",(function(){return i})),t.d(n,"metadata",(function(){return s})),t.d(n,"toc",(function(){return c})),t.d(n,"default",(function(){return l}));var r=t(3),a=t(7),o=(t(0),t(411)),i={title:"Processing Pipelines"},s={unversionedId:"configuration/processing_pipelines",id:"configuration/processing_pipelines",isDocsHomePage:!1,title:"Processing Pipelines",description:"Within a Benthos configuration, in between input and output, is a pipeline section. This section describes an array of processors that are to be applied to all messages, and are not bound to any particular input or output.",source:"@site/docs/configuration/processing_pipelines.md",slug:"/configuration/processing_pipelines",permalink:"/docs/configuration/processing_pipelines",editUrl:"https://github.com/Jeffail/benthos/edit/master/website/docs/configuration/processing_pipelines.md",version:"current",sidebar:"docs",previous:{title:"Field Paths",permalink:"/docs/configuration/field_paths"},next:{title:"Unit Testing",permalink:"/docs/configuration/unit_testing"}},c=[{value:"Multiple Consumers",id:"multiple-consumers",children:[]},{value:"Add a Buffer",id:"add-a-buffer",children:[]}],p={toc:c};function l(e){var n=e.components,t=Object(a.a)(e,["components"]);return Object(o.b)("wrapper",Object(r.a)({},p,t,{components:n,mdxType:"MDXLayout"}),Object(o.b)("p",null,"Within a Benthos configuration, in between ",Object(o.b)("inlineCode",{parentName:"p"},"input")," and ",Object(o.b)("inlineCode",{parentName:"p"},"output"),", is a ",Object(o.b)("inlineCode",{parentName:"p"},"pipeline")," section. This section describes an array of ",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"/docs/components/processors/about"}),"processors")," that are to be applied to ",Object(o.b)("em",{parentName:"p"},"all")," messages, and are not bound to any particular input or output."),Object(o.b)("p",null,"If you have processors that are heavy on CPU and aren't specific to a certain input or output they are best suited for the pipeline section. It is advantageous to use the pipeline section as it allows you to set an explicit number of parallel threads of execution:"),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-yaml"}),"input:\n  resource: foo\n\npipeline:\n  threads: 4\n  processors:\n    - bloblang: |\n        root = this\n        fans = fans.map_each(match {\n          this.obsession > 0.5 => this\n          _ => deleted()\n        })\n\noutput:\n  resource: bar\n")),Object(o.b)("p",null,"If the field ",Object(o.b)("inlineCode",{parentName:"p"},"threads")," is set to ",Object(o.b)("inlineCode",{parentName:"p"},"0")," it will automatically match the number of logical CPUs available."),Object(o.b)("p",null,"By default almost all Benthos sources will utilise as many processing threads as have been configured, which makes horizontal scaling easy. However, this configuration would not be optimal if our input isn't able to utilise >1 processing threads, which will be mentioned in its documentation (",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"/docs/components/inputs/kafka"}),Object(o.b)("inlineCode",{parentName:"a"},"kafka")),", for example)."),Object(o.b)("p",null,"It's also possible that the input source isn't able to provide enough traffic to fully saturate our processing threads. The following patterns can help you to achieve a distribution of work across these processing threads even under those circumstances."),Object(o.b)("h3",{id:"multiple-consumers"},"Multiple Consumers"),Object(o.b)("p",null,"Sometimes our source of data can have many multiple connected clients and will distribute a stream of messages amongst them. In which case it is possible to increase utilisation of parallel processing threads by adding more consumers. This can be done with a ",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"/docs/components/inputs/broker"}),Object(o.b)("inlineCode",{parentName:"a"},"broker")," input"),":"),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-yaml"}),"input:\n  broker:\n    copies: 8\n    inputs:\n      - resource: baz\n\npipeline:\n  threads: 4\n  processors:\n    - bloblang: |\n        root = this\n        fans = fans.map_each(match {\n          this.obsession > 0.5 => this\n          _ => deleted()\n        })\n\noutput:\n  resource: bar\n")),Object(o.b)("p",null,"The disadvantage of this set up is that increasing the number of consuming clients potentially puts unnecessary stress on your data source."),Object(o.b)("h3",{id:"add-a-buffer"},"Add a Buffer"),Object(o.b)("p",null,Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"/docs/components/buffers/about"}),"Buffers")," should be used with caution as they weaken the delivery guarantees of your pipeline. However, they can be very useful for horizontally scaling processing in cases where an input feed is sporadic, as they can level out throughput spikes and provide a backlog of messages during gaps. "),Object(o.b)("pre",null,Object(o.b)("code",Object(r.a)({parentName:"pre"},{className:"language-yaml"}),"input:\n  resource: foo\n\nbuffer:\n  memory:\n    limit: 5000000\n\npipeline:\n  threads: 4\n  processors:\n    - bloblang: |\n        root = this\n        fans = fans.map_each(match {\n          this.obsession > 0.5 => this\n          _ => deleted()\n        })\n\noutput:\n  resource: bar\n")))}l.isMDXComponent=!0},411:function(e,n,t){"use strict";t.d(n,"a",(function(){return u})),t.d(n,"b",(function(){return f}));var r=t(0),a=t.n(r);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function c(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var p=a.a.createContext({}),l=function(e){var n=a.a.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},u=function(e){var n=l(e.components);return a.a.createElement(p.Provider,{value:n},e.children)},b={inlineCode:"code",wrapper:function(e){var n=e.children;return a.a.createElement(a.a.Fragment,{},n)}},d=a.a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,i=e.parentName,p=c(e,["components","mdxType","originalType","parentName"]),u=l(t),d=r,f=u["".concat(i,".").concat(d)]||u[d]||b[d]||o;return t?a.a.createElement(f,s(s({ref:n},p),{},{components:t})):a.a.createElement(f,s({ref:n},p))}));function f(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=d;var s={};for(var c in n)hasOwnProperty.call(n,c)&&(s[c]=n[c]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=t[p];return a.a.createElement.apply(null,i)}return a.a.createElement.apply(null,t)}d.displayName="MDXCreateElement"}}]);